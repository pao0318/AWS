# Generator
The generator is adapted from the U-Net architecture (a popular CNN that is used extensively in the computer vision domain), 
consisting of an “encoder” that maps the single track music data (represented as piano roll images) to a relatively lower dimensional “latent space“ and a ”decoder“ 
that maps the latent space back to multi-track music data.

Here are the inputs provided to the generator:

## Single-track piano roll input: A single melody track of size (32, 128, 1) => (TimeStep, NumPitches, NumTracks) is provided as the input to the generator.

Latent noise vector: A latent noise vector z of dimension (2, 8, 512) is also passed in as input and this is responsible for ensuring that there is a 
distinctive flavor to each output generated by the generator, even when the same input is provided.

Notice from the figure below that the encoding layers of the generator on the left side and decoder layer on on the right side are connected to create a U-shape,
thereby giving the name U-Net to this architecture.
